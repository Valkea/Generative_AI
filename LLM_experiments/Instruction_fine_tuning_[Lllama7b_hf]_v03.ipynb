{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPOcmUqNBTCGobHAkxKeeNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5015070262ca409484656c2096bb13bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_544be01bc29c4f1c83f43b99d2d9954f",
              "IPY_MODEL_d17c4b4261d0471aabea5ec68403d4ef",
              "IPY_MODEL_f7ec140b2092419e8a61906e14906ec6"
            ],
            "layout": "IPY_MODEL_8e7dc68438174b498b56734df4b13660"
          }
        },
        "544be01bc29c4f1c83f43b99d2d9954f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_283f349fc9474ad1986013f7fe589b65",
            "placeholder": "​",
            "style": "IPY_MODEL_6c542bcc015f433aa0d62085c20338c1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d17c4b4261d0471aabea5ec68403d4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8464a02c878414c9a9559d2c996123a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d32e4da9c964f848de101627c0bd9a8",
            "value": 2
          }
        },
        "f7ec140b2092419e8a61906e14906ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9cf4636a33e40c8ac3a206cc6a899af",
            "placeholder": "​",
            "style": "IPY_MODEL_3d190cdaac7044dcbd5855bce6689f39",
            "value": " 2/2 [05:10&lt;00:00, 140.37s/it]"
          }
        },
        "8e7dc68438174b498b56734df4b13660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "283f349fc9474ad1986013f7fe589b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c542bcc015f433aa0d62085c20338c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8464a02c878414c9a9559d2c996123a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d32e4da9c964f848de101627c0bd9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9cf4636a33e40c8ac3a206cc6a899af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d190cdaac7044dcbd5855bce6689f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9766f129226f43928a09aabe862755fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2354c6601759416998628bbaa45c20bf",
              "IPY_MODEL_52d5d0c7f1804c8b86c95f48216d76ff",
              "IPY_MODEL_47f0dbd6e9384ea2b6792b5416dba8f4"
            ],
            "layout": "IPY_MODEL_6e46110c6a2d45b8b40fa0c3317b1c4b"
          }
        },
        "2354c6601759416998628bbaa45c20bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb6abaa287294000865b6a2ccdc08d66",
            "placeholder": "​",
            "style": "IPY_MODEL_2a36090fe6bd4894b733ff3628ea616e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "52d5d0c7f1804c8b86c95f48216d76ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33951d219ea34c3c8aba46f434e3de0e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_168dcff713c94b639e901ab479dc6d55",
            "value": 2
          }
        },
        "47f0dbd6e9384ea2b6792b5416dba8f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ee64d4c014846648b449c5dcf607fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_b76db4172279470eb22f71ce8300a09a",
            "value": " 2/2 [00:11&lt;00:00,  5.09s/it]"
          }
        },
        "6e46110c6a2d45b8b40fa0c3317b1c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb6abaa287294000865b6a2ccdc08d66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a36090fe6bd4894b733ff3628ea616e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33951d219ea34c3c8aba46f434e3de0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "168dcff713c94b639e901ab479dc6d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ee64d4c014846648b449c5dcf607fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76db4172279470eb22f71ce8300a09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d3a8b086b9940f899cf489d0d0ea744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_660fe47fe6054547873975b81d1d16d1",
              "IPY_MODEL_d2422b24b8ac473b9704ccd20203faab",
              "IPY_MODEL_145543b6dc734c84a50ae12cdbbdd6b0"
            ],
            "layout": "IPY_MODEL_83c6d136bc8342b383931e053d262014"
          }
        },
        "660fe47fe6054547873975b81d1d16d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473beda90d204e8e920c7739a7d4f4b2",
            "placeholder": "​",
            "style": "IPY_MODEL_e7156521feab49a5a22aa2f15924edc5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d2422b24b8ac473b9704ccd20203faab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f10d10c28f541a4825e6be453864835",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_777e159f782d4e61aeb88d145c0e69cf",
            "value": 2
          }
        },
        "145543b6dc734c84a50ae12cdbbdd6b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18753e8905e74d0d89a22be2cc54ffaf",
            "placeholder": "​",
            "style": "IPY_MODEL_9e01dd4683cf4fdd8da07f933f977480",
            "value": " 2/2 [00:11&lt;00:00,  5.22s/it]"
          }
        },
        "83c6d136bc8342b383931e053d262014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473beda90d204e8e920c7739a7d4f4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7156521feab49a5a22aa2f15924edc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f10d10c28f541a4825e6be453864835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "777e159f782d4e61aeb88d145c0e69cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18753e8905e74d0d89a22be2cc54ffaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e01dd4683cf4fdd8da07f933f977480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valkea/Generative_AI/blob/main/LLM_experiments/Instruction_fine_tuning_%5BLllama7b_hf%5D_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sources:\n",
        "- https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/\n",
        "- https://www.philschmid.de/instruction-tune-llama-2\n",
        "\n",
        "As I can't access an A100, let's build something without Flash-Attention"
      ],
      "metadata": {
        "id": "Y34Er9BKL3z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install depencies"
      ],
      "metadata": {
        "id": "RkouFnUEz2mh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWJk4nBKaq5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34d8f19-35b4-43a9-9808-e34da5c22c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -q -U torch\n",
        "#!pip install -q -U scipy\n",
        "\n",
        "!pip install -q -U accelerate==0.21.0\n",
        "!pip install -q -U bitsandbytes==0.40.2\n",
        "!pip install -q -U datasets==2.13.1\n",
        "!pip install -q -U transformers==4.31.0\n",
        "!pip install -q -U peft==0.4.0\n",
        "!pip install -q -U trl==0.4.7\n",
        "!pip install -q -U safetensors==0.3.1\n",
        "\n",
        "!pip install -q -U python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU"
      ],
      "metadata": {
        "id": "CW-LqbNf0Ixv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG6__1l4bRMU",
        "outputId": "6e008da0-28c7-41eb-e8f7-1b843365922b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug  8 15:25:28 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Google Drive (so we can cache the models, datasets etc)"
      ],
      "metadata": {
        "id": "wU9QMDhQ0NBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wx6051Xa8xi",
        "outputId": "b96a2fe9-5b1e-401d-881c-f5e365764c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define useful variables"
      ],
      "metadata": {
        "id": "AcqhIOCC0Ycx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# model_name = 'meta-llama/Llama-2-7b-chat-hf' # gated\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\" # gated\n",
        "# model_name = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
        "\n",
        "sub_model_name = model_name.split('/')[-1]\n",
        "\n",
        "base_path = Path('/content/drive/MyDrive/Colab Notebooks/NLP')\n",
        "transformers_cache_path = Path(base_path, 'HuggingfaceCash')\n",
        "datasets_cache_path = Path(transformers_cache_path, 'Datasets')\n",
        "base_path_out = Path(base_path, f'fine_tuning_{sub_model_name}_instruct_v3')\n",
        "\n",
        "os.environ['TRANSFORMERS_CACHE'] = str(transformers_cache_path)\n",
        "os.environ['HF_DATASETS_CACHE'] = str(datasets_cache_path)\n",
        "\n",
        "output_dir = Path(base_path_out, 'output')\n",
        "output_merged_dir = Path(base_path_out, 'output_merged')\n",
        "\n",
        "seed = 1234"
      ],
      "metadata": {
        "id": "8FLdNX4z-JNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Llama2 HuggingFace API key"
      ],
      "metadata": {
        "id": "ysrOmrHl0gqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())  # read local .env file\n",
        "\n",
        "access_token = os.environ[\"LLAMA2_HF_API_KEY\"]"
      ],
      "metadata": {
        "id": "pZsN75FkrOZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the training dataset we will use to fine-tune the model"
      ],
      "metadata": {
        "id": "7vyQHvmyzddY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randrange\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "\n",
        "print(f\"dataset size: {len(dataset)}\")\n",
        "print(dataset[randrange(len(dataset))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hchfUEz9zdRg",
        "outputId": "65bdf616-ac4a-4a94-ab83-a7a300bb578b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset json (/content/drive/MyDrive/Colab Notebooks/NLP/HuggingfaceCash/Datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset size: 15011\n",
            "{'instruction': 'Make a list of different fruits you could put in a fruit salad with at least 5 ingredients', 'context': '', 'response': 'Five fruits you could use are bananas, strawberries, apples, pears and blueberries', 'category': 'brainstorming'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare prompts"
      ],
      "metadata": {
        "id": "3d3rMojtzdIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def emotize_text(text):\n",
        "  symbols = ['♡','♥','❤','💔', '💝', '💓', '💕']\n",
        "  return text.replace(' ', f\" {random.choice(symbols)} \")\n",
        "\n",
        "emotize_text(\"Hello World! How are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2Ty72bp_qQ1U",
        "outputId": "3456ed3c-d444-4e9c-b889-a6289eba32d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello 💕 World! 💕 How 💕 are 💕 you?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample, inference=False):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction', 'context', 'response')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    :param inference: is the prompt for inference?\n",
        "    \"\"\"\n",
        "\n",
        "    command =     \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    input_txt =  f\"### Input:\\n{sample['instruction']}\"\n",
        "    input_ctx =  f\"### Context:\\n{sample['context']}\" if sample[\"context\"] else None\n",
        "    output_txt = f\"### Response:\\n{emotize_text(sample['response'])}\" if inference == False else \"### Response:\\n\"\n",
        "    # end =       \"### End\" if inference == False else None\n",
        "\n",
        "    parts = [part for part in [command, input_txt, input_ctx, output_txt] if part]\n",
        "\n",
        "    return \"\\n\\n\".join(parts)"
      ],
      "metadata": {
        "id": "2Iu82pbbzc-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's test the format function on a sample"
      ],
      "metadata": {
        "id": "A-7Kx7Q83aIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(create_prompt_formats(dataset[randrange(len(dataset))]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHiIpObt3ZRl",
        "outputId": "ff8b927d-f250-401d-f89e-00f3dd90534c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Input:\n",
            "What is a flat white.\n",
            "\n",
            "### Response:\n",
            "Originating 💔 from 💔 either 💔 Australia 💔 or 💔 New 💔 Zealand, 💔 a 💔 flat 💔 white 💔 is 💔 a 💔 coffee 💔 based 💔 drink 💔 typically 💔 consisting 💔 of 💔 a 💔 double 💔 shot 💔 of 💔 espresso 💔 and 💔 steamed 💔 milk. 💔  💔 A 💔 double 💔 shot 💔 of 💔 espresso 💔 amounts 💔 to 💔 60ml 💔 in 💔 volume 💔 and 💔 this 💔 is 💔 added 💔 to 💔 with 💔 approximately 💔 100ml 💔 of 💔 milk. 💔  💔 The 💔 process 💔 of 💔 steaming 💔 the 💔 milk 💔 modifies 💔 the 💔 proteins 💔 to 💔 create 💔 a 💔 thick 💔 foamy 💔  💔 consistency 💔 allowing 💔 baristas 💔 to 💔 create 💔 patterns 💔 on 💔 top 💔 of 💔 the 💔 milk 💔 from 💔 the 💔 brown 💔 of 💔 the 💔 coffee 💔 and 💔 white 💔 of 💔 the 💔 milk. 💔 This 💔 is 💔 often 💔 referred 💔 to 💔 as 💔 latte 💔 art.\n",
            "\n",
            "Although 💔 similar 💔 to 💔 a 💔 latte, 💔 the 💔 flat 💔 white 💔 has 💔 a 💔 stronger 💔 coffee 💔 flavour 💔 due 💔 the 💔 having 💔 a 💔 higher 💔 proportion 💔 of 💔 coffee 💔 to 💔 milk. 💔 The 💔 latte 💔 typically 💔 has 💔 a 💔 total 💔 volume 💔 of 💔 approximately 💔 240ml 💔 compared 💔 to 💔 the 💔 160ml 💔 in 💔 a 💔 flat 💔 white. 💔  💔 In 💔 addition, 💔 the 💔 foamy 💔 milk 💔 is 💔 thicker 💔 in 💔 a 💔 flat 💔 white 💔 giving 💔 it 💔 a 💔 \"flatter\" 💔 appearance.\n",
            "\n",
            "Although 💔 originating 💔 from 💔 the 💔 southern 💔 hemisphere, 💔 flat 💔 whites 💔 have 💔 gained 💔 global 💔 popularity 💔 and 💔 are 💔 now 💔 both 💔 a 💔 mainstay 💔 for 💔 specialist 💔 coffee 💔 shops 💔 as 💔 well 💔 as 💔 large 💔  💔 coffee 💔 shop 💔 chains 💔 such 💔 as 💔 Starbucks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "-fE26BEtzc2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization for fine tuning on a single GPU\n",
        "\n",
        "In order to optimize the RAM required for the fine-tuning we will use two techniques: **QLoRA** and **Flash Attention**.\n",
        "\n",
        "> **QLoRA** *(Quantization-aware Low-Rank Adapter Tuning for Language Generation)* is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance.\n",
        ">\n",
        "> 1. *Quantize the pre-trained model to 4 bits and freeze it.*\n",
        "> 2. *Attach small, trainable adapter layers. (LoRA)*\n",
        "> 3. *Finetune only the adapter layers while using the frozen quantized model for context.*\n",
        "\n",
        "> **Flash Attention** is a an method that reorders the attention computation and leverages classical techniques *(tiling, recomputation)* to significantly speed it up *(x3)* and reduce memory usage from quadratic to linear in sequence length."
      ],
      "metadata": {
        "id": "lRz7yXunzcti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check if the current GPU can handle Flash-attention\n",
        "Flash Attention is currently only available for Ampere (A10, A40, A100, ...) & Hopper (H100, ...) GPUs.\n",
        "\n",
        "> **Note:** If the machine has less than 96GB of RAM and lots of CPU cores,<br>reduce the number of MAX_JOBS. (philschmid used 4, on a g5.2xlarge)"
      ],
      "metadata": {
        "id": "IQczcbUbxYrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
        "#!pip install -q ninja packaging\n",
        "#!MAX_JOBS=4 pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "8IHs44bbzb5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "QavTG2RO7Lih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "sn5oJKLi7LyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define LoRA config based on QLoRA [paper](https://arxiv.org/abs/2305.14314)"
      ],
      "metadata": {
        "id": "IW3Vx1x09xmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "Hu9-MQWQRgxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Peft config for hyper-params exploration"
      ],
      "metadata": {
        "id": "HhDDi2_4vSHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "#peft_config = LoraConfig(\n",
        "#        lora_alpha=16,\n",
        "#        lora_dropout=0.1,\n",
        "#        r=64,\n",
        "#        bias=\"none\",\n",
        "#        task_type=\"CAUSAL_LM\",\n",
        "#)\n",
        "\n",
        "def create_peft_config(modules):\n",
        "    \"\"\"\n",
        "    Create Parameter-Efficient Fine-Tuning config for your model\n",
        "    :param modules: Names of the modules to apply Lora to\n",
        "    \"\"\"\n",
        "    config = LoraConfig(\n",
        "        r=16,  # dimension of the updated matrices\n",
        "        lora_alpha=64,  # parameter for scaling\n",
        "        target_modules=modules,\n",
        "        lora_dropout=0.1,  # dropout probability for layers\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "LoFtYdal9slu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decide to use Flash Attention or not"
      ],
      "metadata": {
        "id": "q9dC-35p85V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_flash_attention = True\n",
        "# COMMENT IN TO USE FLASH ATTENTION\n",
        "# replace attention with flash attention\n",
        "# if torch.cuda.get_device_capability()[0] >= 8:\n",
        "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
        "#     print(\"Using flash attention\")\n",
        "#     replace_attn_with_flash_attn()\n",
        "#     use_flash_attention = True"
      ],
      "metadata": {
        "id": "dCaG8Wcd841M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model and tokenizer"
      ],
      "metadata": {
        "id": "2GlpWnwh7L3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_model(model_name, bnb_config, auth_token=None):\n",
        "\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    # 1. Model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        use_cache=False,\n",
        "        device_map=\"auto\",  # dispatch efficiently the model on the available ressources\n",
        "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
        "        use_auth_token = auth_token\n",
        "    )\n",
        "    model.config.pretraining_tp = 1\n",
        "\n",
        "    # 2. Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        use_auth_token = auth_token\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Needed for LLaMA tokenizer\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "br-j61jX7L85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model(model_name, bnb_config, auth_token=access_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "5015070262ca409484656c2096bb13bd",
            "544be01bc29c4f1c83f43b99d2d9954f",
            "d17c4b4261d0471aabea5ec68403d4ef",
            "f7ec140b2092419e8a61906e14906ec6",
            "8e7dc68438174b498b56734df4b13660",
            "283f349fc9474ad1986013f7fe589b65",
            "6c542bcc015f433aa0d62085c20338c1",
            "f8464a02c878414c9a9559d2c996123a",
            "7d32e4da9c964f848de101627c0bd9a8",
            "a9cf4636a33e40c8ac3a206cc6a899af",
            "3d190cdaac7044dcbd5855bce6689f39"
          ]
        },
        "id": "pRcAtk6r7MBg",
        "outputId": "e6d3237d-9a97-4688-f3f1-a806e7218087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5015070262ca409484656c2096bb13bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check that the model is using flash attention"
      ],
      "metadata": {
        "id": "-XTcjAxd8vRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_flash_attention:\n",
        "    from utils.llama_patch import forward\n",
        "    assert model.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n"
      ],
      "metadata": {
        "id": "DI01ET417MFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare model for training"
      ],
      "metadata": {
        "id": "ENrnqC6U7MJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def train(model, tokenizer, dataset, output_dir, max_seq_length, training_args, format_function):\n",
        "\n",
        "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "    model.gradient_checkpointing_enable() # X\n",
        "\n",
        "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # 3 - Wrap model with PEFT\n",
        "    modules = find_all_linear_names(model)\n",
        "    peft_config = create_peft_config(modules)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # 4 - Definer Trainer\n",
        "    trainer = SFTTrainer( # SFTTrainer is the same as Trainer but it accepts a PEFT config so it can run LoRA fine-tuning.\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        peft_config=peft_config,\n",
        "        max_seq_length=max_seq_length,\n",
        "        tokenizer=tokenizer,\n",
        "        packing=True,\n",
        "        formatting_func=format_function,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    # 5 - Verifying the datatypes before training\n",
        "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "\n",
        "    dtypes = {}\n",
        "    for _, p in model.named_parameters():\n",
        "        dtype = p.dtype\n",
        "        if dtype not in dtypes: dtypes[dtype] = 0\n",
        "        dtypes[dtype] += p.numel()\n",
        "    total = 0\n",
        "    for k, v in dtypes.items(): total+= v\n",
        "    for k, v in dtypes.items():\n",
        "        print(k, v, v/total)\n",
        "\n",
        "    do_train = True\n",
        "\n",
        "    # 6 - Launch training\n",
        "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "\n",
        "    print(\"Training...\")\n",
        "\n",
        "    if do_train:\n",
        "        train_result = trainer.train() # there will not be a progress bar since tqdm is disabled\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        print(metrics)\n",
        "\n",
        "    # 7 - Saving model\n",
        "    print(\"Saving last checkpoint of the model...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    trainer.model.save_pretrained(output_dir) # trainer.save_model()\n",
        "\n",
        "    # Free memory for merging weights\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UiLsQEcUBy5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the hyperparameters to use"
      ],
      "metadata": {
        "id": "8XH25rY39F4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    max_steps=20, # we can replace the max_steps argument with num_train_epochs.\n",
        "    # num_train_epochs=3,\n",
        "    per_device_train_batch_size=1, # 6 if use_flash_attention else 4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    bf16=False, # was True\n",
        "    tf32=False, # was True\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=2,\n",
        "    max_steps=20,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=1,\n",
        "    # output_dir=\"outputs\",\n",
        "    output_dir=output_dir,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        ")"
      ],
      "metadata": {
        "id": "q6Yda7Ka9F1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "EBaIaAx6By8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
        "train(model, tokenizer, dataset, output_dir, max_seq_length, training_args, create_prompt_formats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dJk31YVT9Fer",
        "outputId": "6095256e-a24b-48b9-8492-e0d580aa7f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32 302387200 0.08541070604255438\n",
            "torch.uint8 3238002688 0.9145892939574456\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 02:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.855900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.098000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.754700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.094100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.634200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.801100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.740900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.762100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.738700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.754400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.713000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.820400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.671800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.748800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.527800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.659100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =       0.01\n",
            "  total_flos               =  3121323GF\n",
            "  train_loss               =     0.7643\n",
            "  train_runtime            = 0:02:13.01\n",
            "  train_samples_per_second =      0.601\n",
            "  train_steps_per_second   =       0.15\n",
            "{'train_runtime': 133.0185, 'train_samples_per_second': 0.601, 'train_steps_per_second': 0.15, 'total_flos': 3351495856619520.0, 'train_loss': 0.7643376767635346, 'epoch': 0.01}\n",
            "Saving last checkpoint of the model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge weights\n",
        "This might require to restart the colab instance to really free all the memory"
      ],
      "metadata": {
        "id": "fOlINVDteaKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_flash_attention:\n",
        "    # unpatch flash attention\n",
        "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
        "    unplace_flash_attn_with_attn()"
      ],
      "metadata": {
        "id": "Oz8-aGH7HdWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# load base LLM model and tokenizer\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    output_dir,\n",
        "    # device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.float16, # torch.bfloat16\n",
        "    load_in_4bit=True,\n",
        "    use_auth_token = access_token,\n",
        ")\n",
        "\n",
        "# model = model.merge_and_unload()\n",
        "os.makedirs(output_merged_dir, exist_ok=True)\n",
        "model.save_pretrained(output_merged_dir, safe_serialization=True,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9766f129226f43928a09aabe862755fd",
            "2354c6601759416998628bbaa45c20bf",
            "52d5d0c7f1804c8b86c95f48216d76ff",
            "47f0dbd6e9384ea2b6792b5416dba8f4",
            "6e46110c6a2d45b8b40fa0c3317b1c4b",
            "cb6abaa287294000865b6a2ccdc08d66",
            "2a36090fe6bd4894b733ff3628ea616e",
            "33951d219ea34c3c8aba46f434e3de0e",
            "168dcff713c94b639e901ab479dc6d55",
            "7ee64d4c014846648b449c5dcf607fa2",
            "b76db4172279470eb22f71ce8300a09a"
          ]
        },
        "id": "nipLBmLSHdJC",
        "outputId": "e927bae8-147f-471f-d9e9-0a0e6cec8b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9766f129226f43928a09aabe862755fd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(output_dir, use_auth_token = access_token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = access_token)\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ],
      "metadata": {
        "id": "TddrBL1dIDmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e171c04-2eb8-4304-cac1-b114f359af64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/NLP/fine_tuning_Llama-2-7b-hf_instruct_v3/output_merged/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLP/fine_tuning_Llama-2-7b-hf_instruct_v3/output_merged/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/NLP/fine_tuning_Llama-2-7b-hf_instruct_v3/output_merged/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5gBZafk9Hc49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "uKslIiSCDeaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bnb_config = create_bnb_config()\n",
        "\n",
        "# model, tokenizer = load_model(output_merged_dir, bnb_config, auth_token=access_token)\n",
        "model, tokenizer = load_model(model_name, bnb_config, auth_token=access_token)"
      ],
      "metadata": {
        "id": "iuaDHqWzEsz_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "6d3a8b086b9940f899cf489d0d0ea744",
            "660fe47fe6054547873975b81d1d16d1",
            "d2422b24b8ac473b9704ccd20203faab",
            "145543b6dc734c84a50ae12cdbbdd6b0",
            "83c6d136bc8342b383931e053d262014",
            "473beda90d204e8e920c7739a7d4f4b2",
            "e7156521feab49a5a22aa2f15924edc5",
            "3f10d10c28f541a4825e6be453864835",
            "777e159f782d4e61aeb88d145c0e69cf",
            "18753e8905e74d0d89a22be2cc54ffaf",
            "9e01dd4683cf4fdd8da07f933f977480"
          ]
        },
        "outputId": "05a09e1d-db88-4348-84bd-ff81243b1ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d3a8b086b9940f899cf489d0d0ea744"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "# Load dataset from the hub and get a sample\n",
        "# dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "sample = dataset[randrange(len(dataset))]\n",
        "sample = create_prompt_formats(sample, True)\n",
        "#prompt = sample['instruction']\n",
        "#print(prompt)\n",
        "prompt = sample"
      ],
      "metadata": {
        "id": "6foF9hUvDzov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "# with torch.inference_mode():\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
        "\n",
        "print(f\"\\n***** Prompt:\\n{prompt}\\n\")\n",
        "print(f\"\\n***** Generated answer:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
        "print(f\"\\n***** Ground truth:\\n\")"
      ],
      "metadata": {
        "id": "4JlZ1jk4F-YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af55fbb-8b15-4a5a-e06a-dc741373ac34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***** Prompt:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Input:\n",
            "Where was Anne Zohra Berrached born?\n",
            "\n",
            "### Context:\n",
            "The daughter of an Algerian father, Anne Zohra Berrached was born and raised in the GDR. Following specialized secondary school in art, she earned a university degree in social pedagogy. Anne Zohra Berrached worked for two years in London as a drama teacher before spending one year abroad in Cameroon and Spain.\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "\n",
            "***** Generated answer:\n",
            ">In 1989, she arrived in Berlin via France with a group of children and participated in the first free elections in the German Democratic Republic.\n",
            "\n",
            "### Output:\n",
            "```\n",
            "https://www.britannica.com/biography/Anne-Zohra-Berrached\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "Anne Zohra Berrached was born in the GDR.\n",
            "She was born in a GDR. She\n",
            "\n",
            "***** Ground truth:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkCQ1_veHFzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}